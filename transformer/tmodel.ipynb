{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import copy\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIN_Interaction_Flat(nn.Sequential):\n",
    "    '''\n",
    "        Interaction Network with 2D interaction map\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, **config):\n",
    "        super(BIN_Interaction_Flat, self).__init__()\n",
    "        self.max_d = config['max_drug_seq']\n",
    "        self.max_p = config['max_protein_seq']\n",
    "        self.emb_size = config['emb_size']\n",
    "        self.dropout_rate = config['dropout_rate']\n",
    "        \n",
    "        #densenet\n",
    "        self.scale_down_ratio = config['scale_down_ratio']\n",
    "        self.growth_rate = config['growth_rate']\n",
    "        self.transition_rate = config['transition_rate']\n",
    "        self.num_dense_blocks = config['num_dense_blocks']\n",
    "        self.kernal_dense_size = config['kernal_dense_size']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.input_dim_drug = config['input_dim_drug']\n",
    "        self.input_dim_target = config['input_dim_target']\n",
    "        self.gpus = torch.cuda.device_count()\n",
    "        self.n_layer = 2\n",
    "        #encoder\n",
    "        self.hidden_size = config['emb_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.num_attention_heads = config['num_attention_heads']\n",
    "        self.attention_probs_dropout_prob = config['attention_probs_dropout_prob']\n",
    "        self.hidden_dropout_prob = config['hidden_dropout_prob']\n",
    "        \n",
    "        self.flatten_dim = config['flat_dim'] \n",
    "        \n",
    "        # specialized embedding with positional one\n",
    "        self.demb = Embeddings(self.input_dim_drug, self.emb_size, self.max_d, self.dropout_rate)\n",
    "        self.pemb = Embeddings(self.input_dim_target, self.emb_size, self.max_p, self.dropout_rate)\n",
    "        \n",
    "        self.d_encoder = Encoder_MultipleLayers(self.n_layer, self.hidden_size, self.intermediate_size, self.num_attention_heads, self.attention_probs_dropout_prob, self.hidden_dropout_prob)\n",
    "        self.p_encoder = Encoder_MultipleLayers(self.n_layer, self.hidden_size, self.intermediate_size, self.num_attention_heads, self.attention_probs_dropout_prob, self.hidden_dropout_prob)\n",
    "        \n",
    "        self.icnn = nn.Conv2d(1, 3, 3, padding = 0)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #output layer\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, d, p, d_mask, p_mask):\n",
    "        \n",
    "        ex_d_mask = d_mask.unsqueeze(1).unsqueeze(2)\n",
    "        ex_p_mask = p_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        ex_d_mask = (1.0 - ex_d_mask) * -10000.0\n",
    "        ex_p_mask = (1.0 - ex_p_mask) * -10000.0\n",
    "        \n",
    "        d_emb = self.demb(d) # batch_size x seq_length x embed_size\n",
    "        p_emb = self.pemb(p)\n",
    "\n",
    "        # set output_all_encoded_layers be false, to obtain the last layer hidden states only...\n",
    "        \n",
    "        d_encoded_layers = self.d_encoder(d_emb.float(), ex_d_mask.float())\n",
    "        print(d_encoded_layers.shape)\n",
    "        p_encoded_layers = self.p_encoder(p_emb.float(), ex_p_mask.float())\n",
    "        print(p_encoded_layers.shape)\n",
    "\n",
    "        # repeat to have the same tensor size for aggregation   \n",
    "        d_aug = torch.unsqueeze(d_encoded_layers, 2).repeat(1, 1, self.max_p, 1) # repeat along protein size\n",
    "        p_aug = torch.unsqueeze(p_encoded_layers, 1).repeat(1, self.max_d, 1, 1) # repeat along drug size\n",
    "        \n",
    "        i = d_aug * p_aug # interaction\n",
    "        i_v = i.view(int(self.batch_size/self.gpus), -1, self.max_d, self.max_p) \n",
    "        # batch_size x embed size x max_drug_seq_len x max_protein_seq_len\n",
    "        i_v = torch.sum(i_v, dim = 1)\n",
    "        print(i_v.shape)\n",
    "        i_v = torch.unsqueeze(i_v, 1)\n",
    "        print(i_v.shape)\n",
    "        \n",
    "        i_v = F.dropout(i_v, p = self.dropout_rate)        \n",
    "        \n",
    "        f = self.icnn2(self.icnn1(i_v))\n",
    "        f = self.icnn(i_v)\n",
    "        \n",
    "        print(f.shape)\n",
    "        \n",
    "        f = self.dense_net(f)\n",
    "        print(f.shape)\n",
    "        \n",
    "        f = f.view(int(self.batch_size/self.gpus), -1)\n",
    "        print(f.shape)\n",
    "        \n",
    "        f_encode = torch.cat((d_encoded_layers[:,-1], p_encoded_layers[:,-1]), dim = 1)\n",
    "        \n",
    "        score = self.decoder(torch.cat((f, f_encode), dim = 1))\n",
    "        score = self.decoder(f)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from protein/target, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_size, dropout_rate):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_size, hidden_size)\n",
    "\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_dropout_prob):\n",
    "        super(SelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Attention, self).__init__()\n",
    "        self.self = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = SelfOutput(hidden_size, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(Intermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.relu(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Output(nn.Module):\n",
    "    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n",
    "        super(Output, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Attention(hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob)\n",
    "        self.intermediate = Intermediate(hidden_size, intermediate_size)\n",
    "        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output    \n",
    "\n",
    "    \n",
    "class Encoder_MultipleLayers(nn.Module):\n",
    "    def __init__(self, n_layer, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Encoder_MultipleLayers, self).__init__()\n",
    "        layer = Encoder(hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layer)])    \n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            #if output_all_encoded_layers:\n",
    "            #    all_encoder_layers.append(hidden_states)\n",
    "        #if not output_all_encoded_layers:\n",
    "        #    all_encoder_layers.append(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
